simResult[row,"Rsq"] <- fit.out[3]
if (cvi != "nocv"){
simResult[row,"RMSEsd"] <- fit.out[4]
}
if (cvi != "nocv"){
simResult[row,"R2sd"] <- fit.out[5]
}
simOut <- cbind(grid, simResult) %>% as.data.frame()
MCout[[MC]] <- simOut
}
return(MCout)
}
test <- oneSim(mc.grid, 2)
mc.rmse <- cbind(test[[1]]$RMSE, test[[2]]$RMSE) %>% rowSums()
df <- cbind(mc.grid, mc.rmse)
df$.n <- as.factor(df$.n)
ggplot(df, aes(x=.p, y=mc.rmse, colour = .n)) + geom_line() + facet_grid(~ .cvType)
test[[1]] %>% View
set.seed(1); MLpipeline(makeData(1000,10), cvType = "cv10")
set.seed(2); MLpipeline(makeData(1000,10), cvType = "cv10")
test[[2]] %>% View
set.seed(2); MLpipeline(makeData(1000,10), modelType = "ridge", cvType = "cv10")
set.seed(2); MLpipeline(makeData(1000,10), modelType = "ridge", cvType = "cv10") %>% str()
```{r}
MLpipeline <- function(data, modelType = "lm", cvType, k=NA, perfMet = "RMSE"   ,...){
## One line ML pipeline
## Will train and test a model according to the instructions/CV set up provided
# Data should be a matrix, with the first column the predictor
# model should be from this list: http://topepo.github.io/caret/modelList.html
# cvTypes: "cv" (provide k); "loo"; "boot" (default 25 bootstraps); "nocv"
#          "cv5" (5-fold); "cv10" (10-fold)
# perfMet is the desired performance metric (defaults to RMSE)
# Error handling (lol)
if((cvType %in% c("cv", "loo", "boot", "nocv", "cv5", "cv10")) == FALSE){
stop("you need to provide a cv method that I have pre-coded")
}
if((cvType == "cv") & (is.na(k) == TRUE)){
stop("you asked for cv but didnt provide k")
if((is.numeric(k) == FALSE) | (k <= 1)){
stop("k needs to be a number, greater than 1")
}
# Format data
Yvec <- data[,1]  %>% as.vector()
Xmat <- data[,-1] %>% as.matrix()
# Initialise Cross-Validation
if (cvType == "cv"){
tcontrol <- trainControl(method = "cv", number = k)
} else if (cvType == "cv5"){
tcontrol <- trainControl(method = "cv", number = 5)
} else if (cvType == "cv10"){
tcontrol <- trainControl(method = "cv", number = 10)
} else if (cvType == "loo"){
tcontrol <- trainControl(method = "LOOCV")
} else if (cvType == "boot"){
tcontrol <- trainControl(method = "boot")
} else if (cvType == "nocv"){
tcontrol <- trainControl(method = "none")
}
# Fit the model
fit <- train(x = Xmat, y = Yvec,
method = modelType,
metric = perfMet,
trControl = tcontrol)
out <- fit$results %>% as.vector()
# Results format will vary depending on the modelType
if (modelType == "ridge"){
out <- out[out$RMSE == min(out$RMSE),]
}
return(out)
}
set.seed(2); MLpipeline(makeData(1000,10), modelType = "ridge", cvType = "cv10") %>% str()
mc.grid <- expand.grid(.n = c(40, 100, 150),
.p = c(25, 50, 100),
.cvType = c("cv5", "cv10"),
stringsAsFactors = FALSE)
oneSim <- function(grid,MCnum, ... ){
MCout <- list()
for (MC in 1:MCnum){
simResult <- matrix(NA, nrow = dim(grid)[1], ncol = 4) %>% as.data.frame()
names(simResult) <- c("RMSE", "Rsq", "RMSEsd", "R2sd")
for (row in 1:nrow(grid)){
n = grid[row,".n"]
p = grid[row,".p"]
cvi = grid[row,".cvType"]
set.seed(MC)
dataMat <- makeData(observations = n,predictors = p)
fit.out <- MLpipeline(dataMat, cvType = cvi)
simResult[row,"RMSE"] <- fit.out[2]
simResult[row,"Rsq"] <- fit.out[3]
if (cvi != "nocv"){
simResult[row,"RMSEsd"] <- fit.out[4]
}
if (cvi != "nocv"){
simResult[row,"R2sd"] <- fit.out[5]
}
simOut <- cbind(grid, simResult) %>% as.data.frame()
MCout[[MC]] <- simOut
}
return(MCout)
}
test <- oneSim(mc.grid, 2)
mc.rmse <- cbind(test[[1]]$RMSE, test[[2]]$RMSE) %>% rowSums()
df <- cbind(mc.grid, mc.rmse)
df$.n <- as.factor(df$.n)
ggplot(df, aes(x=.p, y=mc.rmse, colour = .n)) + geom_line() + facet_grid(~ .cvType)
test[[1]] %>% View
test[[2]] %>% View
set.seed(2); MLpipeline(makeData(40,100), cvType = "cv5") %>% str()
mc.grid <- expand.grid(.n = c(40, 100, 150),
.p = c(25, 50, 100),
.cvType = c("cv5", "loo"),
stringsAsFactors = FALSE)
oneSim <- function(grid,MCnum, ... ){
MCout <- list()
for (MC in 1:MCnum){
simResult <- matrix(NA, nrow = dim(grid)[1], ncol = 4) %>% as.data.frame()
names(simResult) <- c("RMSE", "Rsq", "RMSEsd", "R2sd")
for (row in 1:nrow(grid)){
n = grid[row,".n"]
p = grid[row,".p"]
cvi = grid[row,".cvType"]
set.seed(MC)
dataMat <- makeData(observations = n,predictors = p)
fit.out <- MLpipeline(dataMat, cvType = cvi)
simResult[row,"RMSE"] <- fit.out[2]
simResult[row,"Rsq"] <- fit.out[3]
if (cvi != "nocv"){
simResult[row,"RMSEsd"] <- fit.out[4]
}
if (cvi != "nocv"){
simResult[row,"R2sd"] <- fit.out[5]
}
simOut <- cbind(grid, simResult) %>% as.data.frame()
MCout[[MC]] <- simOut
}
return(MCout)
}
test <- oneSim(mc.grid, 2)
mc.rmse <- cbind(test[[1]]$RMSE, test[[2]]$RMSE) %>% rowSums()
df <- cbind(mc.grid, mc.rmse)
df$.n <- as.factor(df$.n)
ggplot(df, aes(x=.p, y=mc.rmse, colour = .n)) + geom_line() + facet_grid(~ .cvType)
set.seed(2); MLpipeline(makeData(40,99), cvType = "cv5") %>% str()
set.seed(2); MLpipeline(makeData(40,100), cvType = "cv5") %>% str()
set.seed(2); MLpipeline(makeData(40,101), cvType = "cv5") %>% str()
40/5
set.seed(2); makeData(40,100) %>% View
set.seed(2); t <- makeData(40,100)
cor(t$Y, t$V100)
str(t)
cor(t[,1], t[,101])
cor(t[,1], [t,-1])
cor(t[,1], t[],-1])
cor(t[,1], t[,-1])
cor(t[,1], t[,-1]) %>% dim(0)
cor(t[,1], t[,-1]) %>% dim()
cor(t[,1], t[,-1]) %>% as.vector()
cor(t[,1], t[,-1]) %>% as.vector() %>% View
cor(t[,1], t[,-1]) %>% as.vector() %>% summary()
detectCores()
install.packages("foreach")
install.packages("iterators")
detectCores()
library(foreach)
detectCores()
library(doMC)
detectCores()
set.seed(1)
rep(1:100)
library(dplyr)
target <- 1:100
library(permute)
target[shuffle(100)]
set.seed(1)
target[shuffle(100)]
set.seed(1)
target[shuffle(100)]
164+200+258
622-6
version
?update.R()
install.packages("lsmeans")
##############
######### Set working directory to the repository you downloaded
setwd("~/Documents/PhD/PhD_Core/Teaching/MLworkshop/dukeWorkshop")
##############
# Load libraries
libs <- c("ggplot2", "RColorBrewer", "glmnet", "caret", "pROC", "permute", "gbm",
"klaR", "plyr", "foreach", "dplyr", "doMC")
lapply(libs, require, character.only = TRUE)
registerDoMC(detectCores()-1)
## Read in (training) data
#     Data contains freesurfer output, along with age and gender, for a bunch
#     of people scanned in the Boston area over the last couple of years.
raw <- read.csv("data/workshopTrain.csv", as.is=TRUE)
raw$X <- NULL
raw$Sex <- as.factor(raw$Sex)
df <- raw[,-1]
cvCtrl <- trainControl(method = "cv", number = 10)   # see help for other CV
mod1 <- train(x= as.matrix(df[,2:390]),
y = as.factor(df$Sex),
method = "lda",
trControl = cvCtrl)
getTrainPerf(mod1) #not bad! this is the average cross-validated performance
mod1
mod2 <- train(x= as.matrix(df[,2:390]),
y = as.factor(df$Sex),
method = "svmLinear",
trControl = cvCtrl)
getTrainPerf(mod1) #not bad! this is the average cross-validated performance
getTrainPerf(mod2) # SVM numerically worse than LDA
mod3 <- train(x= as.matrix(df[,2:390]),
y = as.factor(df$Sex),
method = "knn",
trControl = cvCtrl)
getTrainPerf(mod3) # kNN was fast but lame
mod3
getTrainPerf(mod3) # kNN was fast but lame
set.seed(2)
mod3 <- train(x= as.matrix(df[,2:390]),
y = as.factor(df$Sex),
method = "knn",
trControl = cvCtrl)
getTrainPerf(mod3)
mod3
mod4 <- train(x= as.matrix(df[,2:390]),
y = as.factor(df$Sex),
method = "ctree",
trControl = cvCtrl)
getTrainPerf(mod4) # single decision tree also weaker.
varImp(mod4)
t <- getModelInfo()
m <- list();
for (i in names(t)){
if (t[[i]]$type != "Regression"){
m <- c(m, t[i])
}
names(m)[1:5]
mod4 <- train(x= as.matrix(df[,2:390]),
y = as.factor(df$Sex),
method = "ada",
trControl = cvCtrl)
getTrainPerf(mod4) # single decision tree also weaker.
mod5 <- train(x= as.matrix(df[,2:390]),
y = as.factor(df$Sex),
method = "ctree",
trControl = trainControl(method="cv", number=5))
getTrainPerf(mod5)
mod5 <- train(x= as.matrix(df[,2:390]),
y = as.factor(df$Sex),
method = "ctree",
trControl = trainControl(method="cv", number=7))
getTrainPerf(mod5)
print(mod5)
plot(mod5$finalModel)
plot(mod5$finalModel, type="simple")
svmGrid <- expand.grid(.sigma = c(1, 0.1, 0.05),
.C = c(1.0, 0.5, 0.1))
View(svmGrid)
mod6 <- train(x = as.matrix(df[,2:390]),
y = as.factor(df$Sex),
method = "svmRadial",
tuneGrid = svmGrid,
trControl = cvCtrl)
getTrainPerf(mod6) # wow, that sucked. why?
print(mod6) # Inspect the model summary
mod6.1 <- train(x = as.matrix(df[,2:390]),
y = as.factor(df$Sex),
method = "svmRadial",
trControl = cvCtrl)
mod6.1
testData <- read.csv("data/workshopTest.csv", as.is=TRUE)
testData$X <- NULL
testData$Sex <- as.factor(testData$Sex)
ext <- testData[,-1]
getTrainPerf(mod6.1) # wow, that sucked. why?
sexPredictions <- predict(mod6.1,
newdata = as.matrix(ext[,2:390]))
confusionMatrix(data = sexPredictions, reference = as.factor(ext$Sex))
install.packages("readxl")
setwd("~/Documents/PhD/PhD_Core/Teaching/MLworkshop/dukeWorkshop")
libs <- c("ggplot2", "RColorBrewer", "glmnet", "caret", "pROC", "permute", "gbm",
"klaR", "plyr", "foreach", "dplyr", "doMC", "readxl")
lapply(libs, require, character.only = TRUE)
registerDoMC(detectCores()-1)
raw <- read_excel("raj_workshop_data.xlsx")
raw <- raw[,-1]
complete.cases(raw) %>% table()
predictors <- raw %>% dplyr::select(L_hipp_tail:bdi)
qplot(raw$total_child)
outcome    <- ifelse(outcome == 0, 0, 1)
raw <- read_excel("raj_workshop_data.xlsx")
raw <- raw[,-1]
complete.cases(raw) %>% table()
predictors <- raw %>% dplyr::select(L_hipp_tail:bdi)
outcome    <- raw %>% dplyr::select(total_child)
outcome    <- ifelse(outcome == 0, 0, 1)
train_mat  <- predictors %>% as.matrix %>% scale
dim(train_mat)
length(outcome)
raw <- read_excel("raj_workshop_data.xlsx")
raw <- raw[,-1]
complete.cases(raw) %>% table()
predictors <- raw %>% dplyr::select(L_hipp_tail:bdi)
outcome    <- raw %>% dplyr::select(total_child)
outcome    <- ifelse(outcome == 0, "N", "Y")
train_mat  <- predictors %>% as.matrix %>% scale
cvCtrl <- trainControl(method = "cv", number = 10)   # see help for other CV
mod1 <- train(x= train_mat,
y = as.factor(outcome),
method = "lda",
trControl = cvCtrl)
mod1
binom.test((0.5983*284), 284, p=0.5)
binom.test((round(0.5983*284), 284, p=0.5)
binom.test((round(0.5983*284), 284, p=0.5))
binom.test(round(0.5983*284), 284, p=0.5)
table(outcome)
table(outcome)/284
binom.test(round(0.5983*284), 284, p=0.6267606)
mod2 <- train(x= train_mat,
y = as.factor(outcome),
method = "svm",
trControl = cvCtrl)
mod2 <- train(x= train_mat,
y = as.factor(outcome),
method = "svmLinear",
trControl = cvCtrl)
mod2
mod2 <- train(x= train_mat,
y = as.factor(outcome),
method = "svmLinear",
metric = "Kappa",
trControl = cvCtrl)
mod2
binom.test(round(0.6367634*284), 284, p=0.6267606)
mod2 <- train(x= train_mat,
y = as.factor(outcome),
method = "svmLinear",
trControl = cvCtrl)
plot(mod2)
mod2 <- train(x= train_mat,
y = as.factor(outcome),
method = "svmLinear",
tuneGrid = expand.grid(.C = c(0.001, 0.01, 0.1, 0.5, 1)),
trControl = cvCtrl)
plot(mod2)
getTrainPerf(mod2)
binom.test(round(0.6590494*284), 284, p=0.6267606)
mod2 <- train(x= train_mat,
y = as.factor(outcome),
method = "svmLinear",
tuneGrid = expand.grid(.C = c(0.001, 0.01, 0.1, 0.2,0.4, 0.5, 0.8,1)),
trControl = cvCtrl)
plot(mod2)
plot(varImp(mod2, scale=FALSE))
mod3 <- train(x= train_mat,
y = as.factor(outcome),
method = "svmRadial",
trControl = cvCtrl)
plot(mod3)
mod4 <- train(x= train_mat,
y = as.factor(outcome),
method = "glmnet",
trControl = cvCtrl)
mod4
plot(mod4)
mod4 <- train(x= train_mat,
y = as.factor(outcome),
method = "glmnet",
tuneGrid = expand.grid(.alpha = seq(0,1,by=0.1),
.lambda = seq(0,1,by=0.1)),
trControl = cvCtrl)
plot(mod4)
getTrainPerf()
getTrainPerf(mod4)
raw <- read_excel("raj_workshop_data.xlsx")
raw <- raw[,-1]
complete.cases(raw) %>% table()
raw <- read_excel("raj_workshop_data.xlsx")
raw <- raw[,-1]
complete.cases(raw) %>% table()
predictors <- raw %>% dplyr::select(L_hipp_tail:bdi)
predictors$total_child <- raw$total_child
outcome    <- raw %>% dplyr::select(Life_Binary) %>% as.factor
# outcome    <- ifelse(outcome == 0, "N", "Y")
train_mat  <- predictors %>% as.matrix %>% scale
cvCtrl <- trainControl(method = "cv", number = 10)   # see help for other CV
mod1 <- train(x= train_mat,
y = as.factor(outcome),
method = "lda",
trControl = cvCtrl)
outcome    <- raw %>% dplyr::select(Life_Binary)
predictors <- raw %>% dplyr::select(L_hipp_tail:bdi)
predictors$total_child <- raw$total_child
outcome    <- raw %>% dplyr::select(Life_Binary)
# outcome    <- ifelse(outcome == 0, "N", "Y")
train_mat  <- predictors %>% as.matrix %>% scale
cvCtrl <- trainControl(method = "cv", number = 10)   # see help for other CV
mod1 <- train(x= train_mat,
y = as.factor(outcome),
method = "lda",
trControl = cvCtrl)
setwd("~/Documents/PhD/PhD_Core/Teaching/MLworkshop/dukeWorkshop")
libs <- c("ggplot2", "RColorBrewer", "glmnet", "caret", "pROC", "permute", "gbm",
"klaR", "plyr", "foreach", "dplyr", "doMC", "readxl")
lapply(libs, require, character.only = TRUE)
registerDoMC(detectCores()-1)
raw <- read_excel("raj_workshop_data.xlsx")
raw <- raw[,-1]
complete.cases(raw) %>% table()
predictors <- raw %>% dplyr::select(L_hipp_tail:bdi)
predictors$total_child <- raw$total_child
outcome    <- raw %>% dplyr::select(Life_Binary)
train_mat  <- predictors %>% as.matrix %>% scale
cvCtrl <- trainControl(method = "cv", number = 10)   # see help for other CV
table(outcome)
mod1 <- train(x= train_mat,
y = as.factor(outcome),
method = "lda",
trControl = cvCtrl)
mod2 <- train(x= train_mat,
y = as.factor(outcome),
method = "svmLinear",
tuneGrid = expand.grid(.C = c(0.001, 0.01, 0.1, 0.2,0.4, 0.5, 0.8,1)),
trControl = cvCtrl)
setwd("~/Documents/PhD/PhD_Core/Teaching/MLworkshop/dukeWorkshop")
libs <- c("ggplot2", "RColorBrewer", "glmnet", "caret", "pROC", "permute", "gbm",
"klaR", "plyr", "foreach", "dplyr", "doMC", "readxl")
lapply(libs, require, character.only = TRUE)
registerDoMC(detectCores()-1)
## Read in data
raw <- read_excel("raj_workshop_data.xlsx")
raw <- raw[,-1]
complete.cases(raw) %>% table()
predictors <- raw %>% dplyr::select(L_hipp_tail:bdi)
predictors$total_child <- raw$total_child
outcome    <- raw %>% dplyr::select(Life_Binary)
# outcome    <- ifelse(outcome == 0, "N", "Y")
train_mat  <- predictors %>% as.matrix %>% scale
cvCtrl <- trainControl(method = "cv", number = 10)   # see help for other CV
mod1 <- train(x= train_mat,
y = as.factor(outcome),
method = "lda",
trControl = cvCtrl)
str(outcome)
outcome <- as.factor(outcome)
outcome <- as.vector(outcome)
outcome    <- raw %>% dplyr::select(Life_Binary) %>% unlist
train_mat  <- predictors %>% as.matrix %>% scale
cvCtrl <- trainControl(method = "cv", number = 10)   # see help for other CV
mod1 <- train(x= train_mat,
y = as.factor(outcome),
method = "lda",
trControl = cvCtrl)
mod1
mod2 <- train(x= train_mat,
y = as.factor(outcome),
method = "svmLinear",
tuneGrid = expand.grid(.C = c(0.001, 0.01, 0.1, 0.2,0.4, 0.5, 0.8,1)),
trControl = cvCtrl)
mod2
mod3 <- train(x= train_mat,
y = as.factor(outcome),
method = "svmRadial",
trControl = cvCtrl)
mod3
mod2 <- train(x= train_mat,
y = as.factor(outcome),
method = "svmLinear",
tuneGrid = expand.grid(.C = c(0.001, 0.01, 0.1, 0.2,0.4, 0.5, 0.8,1)),
trControl = cvCtrl)
plot(mod2)
table(outcome)/284
plot(varImp(mod2, scale=FALSE))
binom.test(round(0.7109606*284), 284,p=0.503)
predictors <- raw %>% dplyr::select(L_hipp_tail:gender)
predictors$total_child <- raw$total_child
outcome    <- raw %>% dplyr::select(Life_Binary) %>% unlist
# outcome    <- ifelse(outcome == 0, "N", "Y")
train_mat  <- predictors %>% as.matrix %>% scale
cvCtrl <- trainControl(method = "cv", number = 10)   # see help for other CV
mod1 <- train(x= train_mat,
y = as.factor(outcome),
method = "lda",
trControl = cvCtrl)
mod2 <- train(x= train_mat,
y = as.factor(outcome),
method = "svmLinear",
tuneGrid = expand.grid(.C = c(0.001, 0.01, 0.1, 0.2,0.4, 0.5, 0.8,1)),
trControl = cvCtrl)
print(mod2)
getTrainPerf()
getTrainPerf(mod2)
binom.test(round(0.5640394*284), 284,p=0.503)
qplot(raw$bdi)
plot(varImp(mod2, scale=FALSE))
cor(raw$life_PTSD, raw$curr_PTSD, method = "spearman")
outcome    <- raw %>% dplyr::select(curr_PTSD) %>% unlist
table(outcome)
outcome    <- ifelse(outcome == 3, "Y", "N")
mod2 <- train(x= train_mat,
y = as.factor(outcome),
method = "svmLinear",
tuneGrid = expand.grid(.C = c(0.001, 0.01, 0.1, 0.2,0.4, 0.5, 0.8,1)),
trControl = cvCtrl)
print(mod2)
mod2 <- train(x= train_mat,
y = as.factor(outcome),
method = "svmLinear",
trControl = cvCtrl)
print(mod2)
